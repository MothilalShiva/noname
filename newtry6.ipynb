{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d8fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import time\n",
    "\n",
    "def cluster_material_descriptions(input_file, material_type='FIN', n_clusters=1000, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Cluster material descriptions with aggressive post-merging.\n",
    "    \n",
    "    Parameters:\n",
    "    input_file: path to input CSV file\n",
    "    material_type: material type to filter (default: 'FIN')\n",
    "    n_clusters: initial number of clusters (high value for fine-grained)\n",
    "    similarity_threshold: threshold for merging clusters (higher means less aggressive)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting clustering for {material_type} with initial K={n_clusters}\")\n",
    "    \n",
    "    # Load and filter data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    df_fin = df[df['material_type'] == material_type].copy().reset_index(drop=True)\n",
    "    print(f\"Found {len(df_fin)} {material_type} records\")\n",
    "    \n",
    "    # Text cleaning function\n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s]', ' ', text)  # Replace special chars with space\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    df_fin['clean_desc'] = df_fin['material_description'].apply(clean_text)\n",
    "    descriptions = df_fin['clean_desc'].tolist()\n",
    "    \n",
    "    # Get unique tokens for synonym mapping (sample for efficiency)\n",
    "    print(\"Creating synonym mapping...\")\n",
    "    all_tokens = set()\n",
    "    sample_size = min(10000, len(descriptions))  # Larger sample for better coverage\n",
    "    for desc in descriptions[:sample_size]:\n",
    "        all_tokens.update(desc.split())\n",
    "    all_tokens = list(all_tokens)\n",
    "    \n",
    "    # Create synonym mapping using fuzzy matching with emphasis on short words\n",
    "    token_mapping = {}\n",
    "    for token in all_tokens:\n",
    "        # For short tokens (likely abbreviations), use higher threshold\n",
    "        if len(token) <= 4:\n",
    "            matches = process.extract(token, all_tokens, scorer=fuzz.token_sort_ratio, limit=10)\n",
    "            best_match = token\n",
    "            for match, score, _ in matches:\n",
    "                if score > 90:  # High threshold for short words\n",
    "                    best_match = match\n",
    "                    break\n",
    "        else:\n",
    "            matches = process.extract(token, all_tokens, scorer=fuzz.token_sort_ratio, limit=5)\n",
    "            best_match = token\n",
    "            for match, score, _ in matches:\n",
    "                if score > 85 and len(match) < len(best_match):\n",
    "                    best_match = match\n",
    "        token_mapping[token] = best_match\n",
    "    \n",
    "    # Apply synonym normalization\n",
    "    print(\"Applying synonym normalization...\")\n",
    "    def normalize_tokens(desc):\n",
    "        tokens = desc.split()\n",
    "        normalized = [token_mapping.get(token, token) for token in tokens]\n",
    "        return ' '.join(normalized)\n",
    "    \n",
    "    df_fin['normalized_desc'] = df_fin['clean_desc'].apply(normalize_tokens)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"Generating embeddings...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    batch_size = 2000\n",
    "    embeddings = []\n",
    "    for i in range(0, len(df_fin), batch_size):\n",
    "        batch_descs = df_fin['normalized_desc'].iloc[i:i+batch_size].tolist()\n",
    "        batch_embeddings = model.encode(batch_descs, show_progress_bar=False)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {min(i+batch_size, len(df_fin))}/{len(df_fin)}\")\n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Perform clustering with high K\n",
    "    print(f\"Clustering with K={n_clusters}...\")\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=1000, n_init=3, max_iter=100)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    # Aggressive post-merging based on centroid similarity\n",
    "    print(\"Aggressive merging of similar clusters...\")\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    similarity_matrix = cosine_similarity(centroids)\n",
    "    \n",
    "    # Find clusters to merge\n",
    "    to_merge = {}\n",
    "    for i in range(n_clusters):\n",
    "        for j in range(i+1, n_clusters):\n",
    "            if similarity_matrix[i, j] > similarity_threshold:\n",
    "                if i not in to_merge:\n",
    "                    to_merge[i] = j\n",
    "                else:\n",
    "                    # Chain merging: if i is already merging to k, then j should merge to k too\n",
    "                    to_merge[j] = to_merge[i]\n",
    "    \n",
    "    # Apply merging\n",
    "    new_labels = cluster_labels.copy()\n",
    "    for old_cluster, new_cluster in to_merge.items():\n",
    "        new_labels[new_labels == old_cluster] = new_cluster\n",
    "    \n",
    "    # Renumber clusters to avoid gaps\n",
    "    unique_clusters = np.unique(new_labels)\n",
    "    mapping = {old: new for new, old in enumerate(unique_clusters)}\n",
    "    cluster_labels = np.array([mapping[label] for label in new_labels])\n",
    "    n_clusters_after_merge = len(unique_clusters)\n",
    "    print(f\"Merged from {n_clusters} to {n_clusters_after_merge} clusters\")\n",
    "    \n",
    "    # Handle small clusters by merging them with the nearest large cluster\n",
    "    print(\"Handling small clusters...\")\n",
    "    cluster_counts = pd.Series(cluster_labels).value_counts()\n",
    "    small_clusters = cluster_counts[cluster_counts <= 2].index.tolist()\n",
    "    \n",
    "    for small_cluster in small_clusters:\n",
    "        small_indices = np.where(cluster_labels == small_cluster)[0]\n",
    "        if len(small_indices) == 0:\n",
    "            continue\n",
    "        small_center = np.mean(embeddings[small_indices], axis=0)\n",
    "        min_distance = float('inf')\n",
    "        best_cluster = small_cluster\n",
    "        for cluster_id in range(n_clusters_after_merge):\n",
    "            if cluster_counts.get(cluster_id, 0) > 2:\n",
    "                distance = np.linalg.norm(small_center - centroids[cluster_id])\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    best_cluster = cluster_id\n",
    "        if min_distance < 0.5:\n",
    "            cluster_labels[small_indices] = best_cluster\n",
    "    \n",
    "    # Generate cluster names\n",
    "    print(\"Generating cluster names...\")\n",
    "    cluster_names = {}\n",
    "    for cluster_id in range(n_clusters_after_merge):\n",
    "        cluster_descs = df_fin['material_description'][cluster_labels == cluster_id].tolist()\n",
    "        if not cluster_descs:\n",
    "            cluster_names[cluster_id] = f\"cluster_{cluster_id}\"\n",
    "            continue\n",
    "        all_terms = []\n",
    "        for desc in cluster_descs:\n",
    "            terms = clean_text(desc).split()\n",
    "            terms = [term for term in terms if len(term) > 3]  # Filter short terms\n",
    "            all_terms.extend(terms)\n",
    "        if not all_terms:\n",
    "            first_desc = clean_text(cluster_descs[0])\n",
    "            cluster_names[cluster_id] = \"_\".join(first_desc.split()[:3])\n",
    "            continue\n",
    "        term_counts = pd.Series(all_terms).value_counts()\n",
    "        top_terms = term_counts.head(3).index.tolist()\n",
    "        cluster_names[cluster_id] = \"_\".join(top_terms)\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    df_fin['proposedkey'] = [cluster_names[label] for label in cluster_labels]\n",
    "    df_fin['cluster'] = cluster_labels\n",
    "    \n",
    "    # Save output\n",
    "    output_cols = ['material_number', 'material_type', 'material_description', 'proposedkey', 'cluster']\n",
    "    output_file = f'output_k_{n_clusters}_merged.csv'\n",
    "    df_fin[output_cols].to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print statistics\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    cluster_sizes = df_fin['cluster'].value_counts()\n",
    "    print(f\"\\nCluster Statistics after merging:\")\n",
    "    print(f\"Total clusters: {len(cluster_sizes)}\")\n",
    "    print(f\"Clusters with > 10 items: {len(cluster_sizes[cluster_sizes > 10])}\")\n",
    "    print(f\"Clusters with > 5 items: {len(cluster_sizes[cluster_sizes > 5])}\")\n",
    "    print(f\"Clusters with 1-2 items: {len(cluster_sizes[cluster_sizes <= 2])}\")\n",
    "    print(f\"Processing time: {total_time/60:.2f} minutes\")\n",
    "    print(f\"Output saved to {output_file}\")\n",
    "    \n",
    "    return df_fin\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Adjust parameters here\n",
    "    input_file = 'input.csv'\n",
    "    material_type = 'FIN'\n",
    "    initial_k = 1000  # Start with a high K for fine-grained clusters\n",
    "    similarity_threshold = 0.8  # Lower for more aggressive merging (0.7-0.9)\n",
    "    \n",
    "    result = cluster_material_descriptions(input_file, material_type, n_clusters=initial_k, similarity_threshold=similarity_threshold)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
