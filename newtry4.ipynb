{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b368abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def efficient_clustering_pipeline(input_file, material_type='FIN'):\n",
    "    \"\"\"\n",
    "    Efficient clustering pipeline for large-scale industrial part descriptions\n",
    "    with intelligent K determination and optimized processing\n",
    "    \"\"\"\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load and filter data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    df_fin = df[df['material_type'] == material_type].copy().reset_index(drop=True)\n",
    "    print(f\"Found {len(df_fin)} {material_type} records\")\n",
    "    \n",
    "    # Text cleaning function\n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    # Clean descriptions\n",
    "    df_fin['clean_desc'] = df_fin['material_description'].apply(clean_text)\n",
    "    descriptions = df_fin['clean_desc'].tolist()\n",
    "    \n",
    "    # Load embedding model (using a balanced model for speed/accuracy)\n",
    "    print(\"Loading embedding model...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Generate embeddings in batches with progress tracking\n",
    "    print(\"Generating embeddings...\")\n",
    "    batch_size = 2000\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(df_fin), batch_size):\n",
    "        batch_descs = df_fin['clean_desc'].iloc[i:i+batch_size].tolist()\n",
    "        batch_embeddings = model.encode(batch_descs, show_progress_bar=False)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processed {min(i+batch_size, len(df_fin))}/{len(df_fin)}\")\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Intelligent K determination using sampling and silhouette analysis\n",
    "    print(\"Determining optimal cluster count...\")\n",
    "    \n",
    "    # Method 1: Sample-based estimation\n",
    "    sample_size = min(5000, len(embeddings))\n",
    "    sample_indices = np.random.choice(len(embeddings), sample_size, replace=False)\n",
    "    sample_embeddings = embeddings[sample_indices]\n",
    "    \n",
    "    # Try a range of K values on the sample\n",
    "    k_candidates = [50, 100, 150, 200, 250, 300]\n",
    "    best_k = 100  # Default\n",
    "    best_silhouette = -1\n",
    "    \n",
    "    for k in k_candidates:\n",
    "        if k >= sample_size:\n",
    "            continue\n",
    "            \n",
    "        kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=500, n_init=1)\n",
    "        cluster_labels = kmeans.fit_predict(sample_embeddings)\n",
    "        \n",
    "        # Use a subset for silhouette score to save time\n",
    "        sil_sample_size = min(1000, len(sample_embeddings))\n",
    "        sil_indices = np.random.choice(len(sample_embeddings), sil_sample_size, replace=False)\n",
    "        \n",
    "        try:\n",
    "            silhouette_avg = silhouette_score(sample_embeddings[sil_indices], \n",
    "                                            cluster_labels[sil_indices])\n",
    "            if silhouette_avg > best_silhouette:\n",
    "                best_silhouette = silhouette_avg\n",
    "                best_k = k\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Method 2: Density-based estimation (fallback)\n",
    "    if best_k <= 10:  # If silhouette method didn't work well\n",
    "        nbrs = NearestNeighbors(n_neighbors=10).fit(sample_embeddings)\n",
    "        distances, indices = nbrs.kneighbors(sample_embeddings)\n",
    "        avg_distances = np.mean(distances, axis=1)\n",
    "        density_estimate = np.percentile(avg_distances, 75)\n",
    "        best_k = max(50, min(300, int(sample_size / (density_estimate * 100))))\n",
    "    \n",
    "    print(f\"Selected K: {best_k}\")\n",
    "    \n",
    "    # Perform clustering with optimal K\n",
    "    print(\"Clustering...\")\n",
    "    kmeans = MiniBatchKMeans(n_clusters=best_k, random_state=42, \n",
    "                            batch_size=1000, n_init=3, max_iter=100)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    # Post-processing: Handle small clusters\n",
    "    print(\"Post-processing clusters...\")\n",
    "    cluster_counts = pd.Series(cluster_labels).value_counts()\n",
    "    small_clusters = cluster_counts[cluster_counts <= 2].index.tolist()\n",
    "    \n",
    "    # For each small cluster, check if it should be merged\n",
    "    for small_cluster in small_clusters:\n",
    "        small_indices = np.where(cluster_labels == small_cluster)[0]\n",
    "        \n",
    "        if len(small_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Find the nearest cluster centroid\n",
    "        small_embeddings = embeddings[small_indices]\n",
    "        distances = np.linalg.norm(\n",
    "            kmeans.cluster_centers_ - np.mean(small_embeddings, axis=0), \n",
    "            axis=1\n",
    "        )\n",
    "        nearest_cluster = np.argmin(distances)\n",
    "        \n",
    "        # Only merge if very close (conservative threshold)\n",
    "        if distances[nearest_cluster] < 0.4:\n",
    "            cluster_labels[small_indices] = nearest_cluster\n",
    "    \n",
    "    # Generate meaningful cluster names\n",
    "    print(\"Generating cluster names...\")\n",
    "    cluster_names = {}\n",
    "    \n",
    "    for cluster_id in range(best_k):\n",
    "        cluster_mask = (cluster_labels == cluster_id)\n",
    "        cluster_descs = df_fin['material_description'][cluster_mask].tolist()\n",
    "        \n",
    "        if not cluster_descs:\n",
    "            cluster_names[cluster_id] = f\"cluster_{cluster_id}\"\n",
    "            continue\n",
    "            \n",
    "        # Extract most common meaningful terms\n",
    "        all_terms = []\n",
    "        for desc in cluster_descs:\n",
    "            terms = clean_text(desc).split()\n",
    "            # Filter out very short terms\n",
    "            terms = [term for term in terms if len(term) > 3]\n",
    "            all_terms.extend(terms)\n",
    "        \n",
    "        if not all_terms:\n",
    "            # Fallback to first few words of first description\n",
    "            first_desc = clean_text(cluster_descs[0])\n",
    "            cluster_names[cluster_id] = \"_\".join(first_desc.split()[:3])\n",
    "            continue\n",
    "            \n",
    "        # Get most frequent terms\n",
    "        term_counts = pd.Series(all_terms).value_counts()\n",
    "        top_terms = term_counts.head(3).index.tolist()\n",
    "        cluster_names[cluster_id] = \"_\".join(top_terms)\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    df_fin['proposedkey'] = [cluster_names[label] for label in cluster_labels]\n",
    "    df_fin['cluster'] = cluster_labels\n",
    "    \n",
    "    # Save output\n",
    "    output_cols = ['material_number', 'material_type', 'material_description', 'proposedkey', 'cluster']\n",
    "    df_fin[output_cols].to_csv('output.csv', index=False)\n",
    "    \n",
    "    # Print completion message\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Processing completed in {total_time/60:.2f} minutes\")\n",
    "    print(f\"Created {best_k} clusters\")\n",
    "    print(f\"Output saved to output.csv\")\n",
    "    \n",
    "    return df_fin\n",
    "\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    df_result = efficient_clustering_pipeline('input.csv', 'FIN')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
