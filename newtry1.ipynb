{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91317ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re, string, random, math\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from kneed import KneeLocator   # for elbow detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Parameters\n",
    "PARAMS = {\n",
    "    \"INPUT_FILE\": \"input.csv\",\n",
    "    \"OUTPUT_FILE\": \"output.csv\",\n",
    "    \"MATERIAL_TYPE\": \"FIN\",            # change this when processing others\n",
    "    \"RANDOM_STATE\": 42,\n",
    "    \"MAX_K\": 250,                      # cap per parent bucket\n",
    "    \"ELBOW_SAMPLE\": 15,                # points for elbow curve\n",
    "    \"SVD_COMPONENTS\": 200,             # set to None to disable dimensionality reduction\n",
    "    \"SINGLETON_SIM_THRESHOLD\": 0.45,   # cosine threshold for attaching outliers\n",
    "    \"SINGLETON_SIZE_THRESHOLD\": 1,     # cluster sizes <= this = singleton\n",
    "}\n",
    "\n",
    "# Expand manually when you learn more about materials\n",
    "PARENT_KEYWORDS = [\"ASSY\", \"ASSEMBLY\", \"DB\", \"MOTOR\", \"BEARING\", \"CABLE\", \"VALVE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8385bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Data\n",
    "df = pd.read_csv(PARAMS[\"INPUT_FILE\"])\n",
    "df = df[df[\"material_type\"] == PARAMS[\"MATERIAL_TYPE\"]].copy().reset_index(drop=True)\n",
    "df[\"material_description\"] = df[\"material_description\"].fillna(\"\")\n",
    "\n",
    "print(f\"Loaded {len(df)} rows for material_type={PARAMS['MATERIAL_TYPE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef07b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Cleaning Function\n",
    "def clean_text(text):\n",
    "    text = str(text).upper().strip()\n",
    "    # Keep only alphanumeric + space\n",
    "    text = re.sub(r\"[^A-Z0-9 ]\", \" \", text)\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_desc\"] = df[\"material_description\"].apply(clean_text)\n",
    "df.loc[df[\"clean_desc\"].isin([\"\", \".\", \"-\", \"?\", \" \"]), \"clean_desc\"] = \"MISC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075714a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Parent Bucket Assignment\n",
    "def get_parent(text):\n",
    "    if text == \"MISC\":\n",
    "        return \"MISC\"\n",
    "    for kw in PARENT_KEYWORDS:\n",
    "        if re.search(rf\"\\b{kw}\\b\", text):\n",
    "            return kw\n",
    "    return \"OTHER\"\n",
    "\n",
    "df[\"parent_bucket\"] = df[\"clean_desc\"].apply(get_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b12c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Elbow Function\n",
    "def find_optimal_k(X, max_k=50, sample_points=15, plot=False, bucket_name=\"\"):\n",
    "    k_values = np.linspace(2, max_k, sample_points, dtype=int)\n",
    "    inertias = []\n",
    "\n",
    "    for k in k_values:\n",
    "        km = MiniBatchKMeans(n_clusters=k, random_state=PARAMS[\"RANDOM_STATE\"], batch_size=2048)\n",
    "        km.fit(X)\n",
    "        inertias.append(km.inertia_)\n",
    "\n",
    "    kn = KneeLocator(k_values, inertias, curve=\"convex\", direction=\"decreasing\")\n",
    "    best_k = kn.knee if kn.knee else int(np.median(k_values))\n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.plot(k_values, inertias, \"bo-\")\n",
    "        if kn.knee:\n",
    "            plt.axvline(kn.knee, color=\"r\", linestyle=\"--\", label=f\"Elbow={kn.knee}\")\n",
    "        plt.title(f\"Elbow Curve for {bucket_name}\")\n",
    "        plt.xlabel(\"Number of clusters (k)\")\n",
    "        plt.ylabel(\"Inertia\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Clustering + Singleton Handling\n",
    "final_clusters = []\n",
    "\n",
    "for bucket, group in df.groupby(\"parent_bucket\"):\n",
    "    print(f\"Processing bucket: {bucket}, size={len(group)}\")\n",
    "\n",
    "    # trivial buckets\n",
    "    if len(group) < 5 or bucket == \"MISC\":\n",
    "        group[\"proposedkey\"] = bucket\n",
    "        final_clusters.append(group)\n",
    "        continue\n",
    "\n",
    "    # TF-IDF vectorization\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2,4), analyzer=\"char_wb\", min_df=2)\n",
    "    X = vectorizer.fit_transform(group[\"clean_desc\"])\n",
    "\n",
    "    # optional SVD for large vocab\n",
    "    if PARAMS[\"SVD_COMPONENTS\"]:\n",
    "        svd = TruncatedSVD(n_components=min(PARAMS[\"SVD_COMPONENTS\"], X.shape[1]-1))\n",
    "        X = svd.fit_transform(X)\n",
    "    else:\n",
    "        X = X.toarray()\n",
    "\n",
    "    # elbow\n",
    "    max_k = min(PARAMS[\"MAX_K\"], len(group)//2)\n",
    "    best_k = find_optimal_k(X, max_k=max_k, sample_points=PARAMS[\"ELBOW_SAMPLE\"], plot=False, bucket_name=bucket)\n",
    "    print(f\"  -> Chosen k = {best_k}\")\n",
    "\n",
    "    # cluster\n",
    "    km = MiniBatchKMeans(n_clusters=best_k, random_state=PARAMS[\"RANDOM_STATE\"], batch_size=2048)\n",
    "    cluster_labels = km.fit_predict(X)\n",
    "    group[\"cluster_id\"] = cluster_labels\n",
    "\n",
    "    # assign names (most frequent token in cluster)\n",
    "    cluster_names = {}\n",
    "    for cid in np.unique(cluster_labels):\n",
    "        texts = group[group[\"cluster_id\"] == cid][\"clean_desc\"]\n",
    "        tokens = \" \".join(texts).split()\n",
    "        cname = pd.Series(tokens).value_counts().index[0] if len(tokens) > 0 else bucket\n",
    "        cluster_names[cid] = cname\n",
    "\n",
    "    group[\"proposedkey\"] = group[\"cluster_id\"].map(cluster_names)\n",
    "\n",
    "    # handle singletons: attach to nearest cluster if similar enough\n",
    "    cluster_sizes = group[\"cluster_id\"].value_counts()\n",
    "    singletons = cluster_sizes[cluster_sizes <= PARAMS[\"SINGLETON_SIZE_THRESHOLD\"]].index\n",
    "\n",
    "    if len(singletons) > 0:\n",
    "        print(f\"  -> Handling {len(singletons)} singleton clusters\")\n",
    "        centroids = km.cluster_centers_\n",
    "        X_norm = normalize(X)\n",
    "        for cid in singletons:\n",
    "            idxs = group[group[\"cluster_id\"] == cid].index\n",
    "            for idx in idxs:\n",
    "                sims = cosine_similarity(X_norm[idx].reshape(1, -1), centroids)\n",
    "                best_target = np.argmax(sims)\n",
    "                if sims[0, best_target] >= PARAMS[\"SINGLETON_SIM_THRESHOLD\"]:\n",
    "                    group.at[idx, \"cluster_id\"] = best_target\n",
    "                    group.at[idx, \"proposedkey\"] = cluster_names[best_target]\n",
    "\n",
    "    final_clusters.append(group)\n",
    "\n",
    "df_final = pd.concat(final_clusters).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fdc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save Final Output\n",
    "df_final = df_final[[\"material_number\", \"material_type\", \"material_description\", \"proposedkey\"]]\n",
    "df_final = df_final.sort_values(by=[\"proposedkey\", \"material_number\"]).reset_index(drop=True)\n",
    "df_final.to_csv(PARAMS[\"OUTPUT_FILE\"], index=False)\n",
    "\n",
    "print(f\"Final output saved to {PARAMS['OUTPUT_FILE']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
