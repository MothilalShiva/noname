{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87324b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import time\n",
    "\n",
    "def cluster_material_descriptions(input_file, material_type='FIN', n_clusters=300):\n",
    "    \"\"\"\n",
    "    Cluster material descriptions with adjustable K value\n",
    "    \n",
    "    Parameters:\n",
    "    input_file: path to input CSV file\n",
    "    material_type: material type to filter (default: 'FIN')\n",
    "    n_clusters: number of clusters to create (adjustable parameter)\n",
    "    \"\"\"\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting clustering for {material_type} with K={n_clusters}\")\n",
    "    \n",
    "    # Load and filter data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    df_fin = df[df['material_type'] == material_type].copy().reset_index(drop=True)\n",
    "    print(f\"Found {len(df_fin)} {material_type} records\")\n",
    "    \n",
    "    # Text cleaning function\n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    # Clean descriptions\n",
    "    df_fin['clean_desc'] = df_fin['material_description'].apply(clean_text)\n",
    "    descriptions = df_fin['clean_desc'].tolist()\n",
    "    \n",
    "    # Get unique tokens for synonym mapping (sample for efficiency)\n",
    "    print(\"Creating synonym mapping...\")\n",
    "    all_tokens = set()\n",
    "    sample_size = min(5000, len(descriptions))\n",
    "    for desc in descriptions[:sample_size]:\n",
    "        all_tokens.update(desc.split())\n",
    "    all_tokens = list(all_tokens)\n",
    "    \n",
    "    # Create synonym mapping using fuzzy matching\n",
    "    token_mapping = {}\n",
    "    for token in all_tokens:\n",
    "        # Find similar tokens\n",
    "        matches = process.extract(token, all_tokens, scorer=fuzz.token_sort_ratio, limit=5)\n",
    "        best_match = token\n",
    "        for match, score, _ in matches:\n",
    "            if score > 85 and len(match) < len(best_match):  # Prefer shorter forms\n",
    "                best_match = match\n",
    "        token_mapping[token] = best_match\n",
    "    \n",
    "    # Apply synonym normalization\n",
    "    print(\"Applying synonym normalization...\")\n",
    "    def normalize_tokens(desc):\n",
    "        tokens = desc.split()\n",
    "        normalized = [token_mapping.get(token, token) for token in tokens]\n",
    "        return ' '.join(normalized)\n",
    "    \n",
    "    df_fin['normalized_desc'] = df_fin['clean_desc'].apply(normalize_tokens)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"Generating embeddings...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    batch_size = 2000\n",
    "    embeddings = []\n",
    "    for i in range(0, len(df_fin), batch_size):\n",
    "        batch_descs = df_fin['normalized_desc'].iloc[i:i+batch_size].tolist()\n",
    "        batch_embeddings = model.encode(batch_descs, show_progress_bar=False)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        if (i // batch_size) % 5 == 0:  # Print progress every 5 batches\n",
    "            print(f\"Processed {min(i+batch_size, len(df_fin))}/{len(df_fin)}\")\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Perform clustering with specified K\n",
    "    print(f\"Clustering with K={n_clusters}...\")\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, \n",
    "                            batch_size=1000, n_init=3, max_iter=100)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    # Post-processing: Handle small clusters\n",
    "    print(\"Post-processing clusters...\")\n",
    "    cluster_counts = pd.Series(cluster_labels).value_counts()\n",
    "    small_clusters = cluster_counts[cluster_counts <= 2].index.tolist()\n",
    "    \n",
    "    # For each small cluster, check if it should be merged\n",
    "    for small_cluster in small_clusters:\n",
    "        small_indices = np.where(cluster_labels == small_cluster)[0]\n",
    "        \n",
    "        if len(small_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get centroid of small cluster\n",
    "        if len(small_indices) == 1:\n",
    "            small_center = embeddings[small_indices[0]]\n",
    "        else:\n",
    "            small_center = np.mean(embeddings[small_indices], axis=0)\n",
    "        \n",
    "        # Find closest large cluster\n",
    "        min_distance = float('inf')\n",
    "        best_cluster = small_cluster\n",
    "        \n",
    "        for cluster_id in range(n_clusters):\n",
    "            if cluster_counts.get(cluster_id, 0) > 2:  # Only consider large clusters\n",
    "                distance = np.linalg.norm(small_center - kmeans.cluster_centers_[cluster_id])\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    best_cluster = cluster_id\n",
    "        \n",
    "        # Only merge if very close (conservative threshold)\n",
    "        if min_distance < 0.4:\n",
    "            cluster_labels[small_indices] = best_cluster\n",
    "    \n",
    "    # Generate cluster names\n",
    "    print(\"Generating cluster names...\")\n",
    "    cluster_names = {}\n",
    "    \n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_mask = (cluster_labels == cluster_id)\n",
    "        cluster_descs = df_fin['material_description'][cluster_mask].tolist()\n",
    "        \n",
    "        if not cluster_descs:\n",
    "            cluster_names[cluster_id] = f\"cluster_{cluster_id}\"\n",
    "            continue\n",
    "            \n",
    "        # Extract most common meaningful terms\n",
    "        all_terms = []\n",
    "        for desc in cluster_descs:\n",
    "            terms = clean_text(desc).split()\n",
    "            # Filter out very short terms\n",
    "            terms = [term for term in terms if len(term) > 3]\n",
    "            all_terms.extend(terms)\n",
    "        \n",
    "        if not all_terms:\n",
    "            # Fallback to first few words of first description\n",
    "            first_desc = clean_text(cluster_descs[0])\n",
    "            cluster_names[cluster_id] = \"_\".join(first_desc.split()[:3])\n",
    "            continue\n",
    "            \n",
    "        # Get most frequent terms\n",
    "        term_counts = pd.Series(all_terms).value_counts()\n",
    "        top_terms = term_counts.head(3).index.tolist()\n",
    "        cluster_names[cluster_id] = \"_\".join(top_terms)\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    df_fin['proposedkey'] = [cluster_names[label] for label in cluster_labels]\n",
    "    df_fin['cluster'] = cluster_labels\n",
    "    \n",
    "    # Save output\n",
    "    output_cols = ['material_number', 'material_type', 'material_description', 'proposedkey', 'cluster']\n",
    "    output_file = f'output_k_{n_clusters}.csv'\n",
    "    df_fin[output_cols].to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print completion message\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # Print cluster statistics\n",
    "    cluster_sizes = df_fin['cluster'].value_counts()\n",
    "    print(f\"\\nCluster Statistics for K={n_clusters}:\")\n",
    "    print(f\"Total clusters: {len(cluster_sizes)}\")\n",
    "    print(f\"Clusters with > 10 items: {len(cluster_sizes[cluster_sizes > 10])}\")\n",
    "    print(f\"Clusters with > 5 items: {len(cluster_sizes[cluster_sizes > 5])}\")\n",
    "    print(f\"Clusters with 1-2 items: {len(cluster_sizes[cluster_sizes <= 2])}\")\n",
    "    print(f\"Largest cluster size: {cluster_sizes.max()}\")\n",
    "    print(f\"Average cluster size: {cluster_sizes.mean():.2f}\")\n",
    "    \n",
    "    print(f\"\\nProcessing completed in {total_time/60:.2f} minutes\")\n",
    "    print(f\"Output saved to {output_file}\")\n",
    "    \n",
    "    return df_fin\n",
    "\n",
    "# Run the clustering with a single K value\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your desired K value here\n",
    "    K_VALUE = 1000  # Change this to any value you want to try\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"RUNNING CLUSTERING WITH K = {K_VALUE}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = cluster_material_descriptions('input.csv', 'FIN', n_clusters=K_VALUE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
