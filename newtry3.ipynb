{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from kneed import KneeLocator\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the pre-trained sentence transformer model for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Read the input CSV\n",
    "df = pd.read_csv('input.csv')\n",
    "# Filter for material_type 'FIN'\n",
    "df_fin = df[df['material_type'] == 'FIN'].copy()\n",
    "descriptions = df_fin['material_description'].astype(str).tolist()\n",
    "\n",
    "# Step 1: Preprocessing function for text cleaning\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove punctuation and special chars\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Clean descriptions\n",
    "cleaned_descriptions = [clean_text(desc) for desc in descriptions]\n",
    "\n",
    "# Step 2: Automated synonym discovery\n",
    "def get_unique_tokens(descriptions):\n",
    "    tokens = set()\n",
    "    for desc in descriptions:\n",
    "        tokens.update(desc.split())\n",
    "    return list(tokens)\n",
    "\n",
    "unique_tokens = get_unique_tokens(cleaned_descriptions)\n",
    "\n",
    "# Fuzzy matching for tokens\n",
    "def find_fuzzy_matches(tokens, score_threshold=90):\n",
    "    synonym_map = {}\n",
    "    for token in tokens:\n",
    "        matches = process.extract(token, tokens, scorer=fuzz.token_sort_ratio, limit=10)\n",
    "        for match, score, _ in matches:\n",
    "            if score >= score_threshold and token != match:\n",
    "                if token not in synonym_map:\n",
    "                    synonym_map[token] = set()\n",
    "                synonym_map[token].add(match)\n",
    "    return synonym_map\n",
    "\n",
    "fuzzy_synonym_map = find_fuzzy_matches(unique_tokens, score_threshold=90)\n",
    "\n",
    "# Embedding-based similarity for tokens\n",
    "def find_embedding_similarities(tokens, model, similarity_threshold=0.7):\n",
    "    token_embeddings = model.encode(tokens)\n",
    "    similarity_matrix = cosine_similarity(token_embeddings)\n",
    "    synonym_map = {}\n",
    "    for i, token1 in enumerate(tokens):\n",
    "        for j, token2 in enumerate(tokens):\n",
    "            if i != j and similarity_matrix[i][j] >= similarity_threshold:\n",
    "                if token1 not in synonym_map:\n",
    "                    synonym_map[token1] = set()\n",
    "                synonym_map[token1].add(token2)\n",
    "    return synonym_map\n",
    "\n",
    "embedding_synonym_map = find_embedding_similarities(unique_tokens, model, similarity_threshold=0.7)\n",
    "\n",
    "# Combine fuzzy and embedding synonym maps\n",
    "combined_synonym_map = {}\n",
    "for token in unique_tokens:\n",
    "    combined_set = set()\n",
    "    if token in fuzzy_synonym_map:\n",
    "        combined_set.update(fuzzy_synonym_map[token])\n",
    "    if token in embedding_synonym_map:\n",
    "        combined_set.update(embedding_synonym_map[token])\n",
    "    if combined_set:\n",
    "        combined_synonym_map[token] = combined_set\n",
    "\n",
    "# Create a mapping from each token to a representative token (choose the most frequent or shortest)\n",
    "def get_representative_token(token, synonym_map, token_freq):\n",
    "    if token not in synonym_map:\n",
    "        return token\n",
    "    synonyms = synonym_map[token]\n",
    "    synonyms.add(token)\n",
    "    # Choose the shortest token as representative for simplicity\n",
    "    return min(synonyms, key=len)\n",
    "\n",
    "# Calculate token frequencies for representative choice\n",
    "token_freq = {}\n",
    "for desc in cleaned_descriptions:\n",
    "    for token in desc.split():\n",
    "        token_freq[token] = token_freq.get(token, 0) + 1\n",
    "\n",
    "# Build final token to representative mapping\n",
    "token_to_rep = {}\n",
    "for token in unique_tokens:\n",
    "    rep = get_representative_token(token, combined_synonym_map, token_freq)\n",
    "    token_to_rep[token] = rep\n",
    "\n",
    "# Apply synonym normalization to descriptions\n",
    "def normalize_description(desc, token_map):\n",
    "    tokens = desc.split()\n",
    "    normalized_tokens = [token_map.get(token, token) for token in tokens]\n",
    "    return ' '.join(normalized_tokens)\n",
    "\n",
    "normalized_descriptions = [normalize_description(desc, token_to_rep) for desc in cleaned_descriptions]\n",
    "\n",
    "# Step 3: Vectorize normalized descriptions\n",
    "embeddings = model.encode(normalized_descriptions)\n",
    "\n",
    "# Step 4: Determine optimal K using elbow method\n",
    "def find_optimal_k(embeddings, max_k=50):\n",
    "    distortions = []\n",
    "    k_range = range(2, max_k+1)\n",
    "    for k in k_range:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=1000)\n",
    "        kmeans.fit(embeddings)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    knee_locator = KneeLocator(k_range, distortions, curve='convex', direction='decreasing')\n",
    "    return knee_locator.knee\n",
    "\n",
    "optimal_k = find_optimal_k(embeddings, max_k=50)\n",
    "if optimal_k is None:\n",
    "    optimal_k = 20  # Default if knee not found\n",
    "\n",
    "# Step 5: Apply MiniBatchKMeans clustering\n",
    "kmeans = MiniBatchKMeans(n_clusters=optimal_k, random_state=42, batch_size=1000)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Step 6: Post-processing for singletons\n",
    "cluster_sizes = pd.Series(cluster_labels).value_counts()\n",
    "singleton_clusters = cluster_sizes[cluster_sizes <= 2].index.tolist()\n",
    "\n",
    "# For each singleton, check similarity to nearest large cluster\n",
    "new_labels = cluster_labels.copy()\n",
    "for singleton in singleton_clusters:\n",
    "    singleton_indices = np.where(cluster_labels == singleton)[0]\n",
    "    if len(singleton_indices) == 0:\n",
    "        continue\n",
    "    singleton_embedding = embeddings[singleton_indices[0]].reshape(1, -1)\n",
    "    # Get all non-singleton clusters\n",
    "    large_clusters = [c for c in range(optimal_k) if c not in singleton_clusters]\n",
    "    if not large_clusters:\n",
    "        continue\n",
    "    large_centroids = centroids[large_clusters]\n",
    "    similarities = cosine_similarity(singleton_embedding, large_centroids)\n",
    "    max_similarity = np.max(similarities)\n",
    "    nearest_cluster = large_clusters[np.argmax(similarities)]\n",
    "    if max_similarity >= 0.9:  High threshold to avoid irrelevant merges\n",
    "        new_labels[singleton_indices] = nearest_cluster\n",
    "    else:\n",
    "        # Keep as singleton\n",
    "        pass\n",
    "\n",
    "# Update cluster labels after post-processing\n",
    "cluster_labels = new_labels\n",
    "\n",
    "# Step 7: Generate cluster names using c-TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def generate_cluster_names(descriptions, cluster_labels):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    cluster_names = {}\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_descriptions = [descriptions[i] for i in range(len(descriptions)) if cluster_labels[i] == cluster]\n",
    "        if not cluster_descriptions:\n",
    "            cluster_names[cluster] = \"Unknown\"\n",
    "            continue\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=5)\n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform(cluster_descriptions)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            scores = tfidf_matrix.sum(axis=0).A1\n",
    "            top_features = [feature_names[i] for i in np.argsort(scores)[-3:][::-1]]\n",
    "            cluster_name = \"_\".join(top_features)\n",
    "        except:\n",
    "            cluster_name = \" \".join(cluster_descriptions[0].split()[:3])\n",
    "        cluster_names[cluster] = cluster_name\n",
    "    return cluster_names\n",
    "\n",
    "cluster_names = generate_cluster_names(descriptions, cluster_labels)\n",
    "\n",
    "# Step 8: Assign proposedkey and cluster to dataframe\n",
    "df_fin['proposedkey'] = [cluster_names[label] for label in cluster_labels]\n",
    "df_fin['cluster'] = cluster_labels\n",
    "\n",
    "# Step 9: Save to output CSV\n",
    "df_fin.to_csv('output.csv', index=False)\n",
    "\n",
    "print(\"Processing complete. Output saved to output.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
