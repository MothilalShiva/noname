{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645db434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "\n",
    "def advanced_clustering(input_file, material_type='FIN', n_clusters=800, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Advanced clustering with automated validation and dynamic term discovery\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting advanced clustering for {material_type}\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(input_file)\n",
    "    df_fin = df[df['material_type'] == material_type].copy().reset_index(drop=True)\n",
    "    print(f\"Found {len(df_fin)} {material_type} records\")\n",
    "    \n",
    "    # Text cleaning with language preservation\n",
    "    def clean_text(text):\n",
    "        text = str(text)\n",
    "        text = re.sub(r'[^\\w\\säöüßàâçéèêëîïôùûüÿæøå]', ' ', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text.lower()\n",
    "    \n",
    "    df_fin['clean_desc'] = df_fin['material_description'].apply(clean_text)\n",
    "    descriptions = df_fin['clean_desc'].tolist()\n",
    "    \n",
    "    # Dynamically discover important terms from the dataset\n",
    "    print(\"Discovering important terms from dataset...\")\n",
    "    def discover_industrial_terms(descriptions, top_n=50):\n",
    "        # Use TF-IDF to find important terms\n",
    "        vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(descriptions)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        term_importance = np.array(tfidf_matrix.sum(axis=0)).flatten()\n",
    "        \n",
    "        # Get top terms\n",
    "        top_indices = np.argsort(term_importance)[-top_n:]\n",
    "        return set(feature_names[i] for i in top_indices if term_importance[i] > 0)\n",
    "    \n",
    "    industrial_terms = discover_industrial_terms(descriptions)\n",
    "    print(f\"Discovered {len(industrial_terms)} important terms: {list(industrial_terms)[:10]}...\")\n",
    "    \n",
    "    # Use multilingual embedding model\n",
    "    print(\"Loading multilingual embedding model...\")\n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"Generating multilingual embeddings...\")\n",
    "    batch_size = 1000\n",
    "    embeddings = []\n",
    "    for i in range(0, len(df_fin), batch_size):\n",
    "        batch_descs = df_fin['clean_desc'].iloc[i:i+batch_size].tolist()\n",
    "        batch_embeddings = model.encode(batch_descs, show_progress_bar=False)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {min(i+batch_size, len(df_fin))}/{len(df_fin)}\")\n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Create domain-aware features using TF-IDF\n",
    "    print(\"Creating domain-aware features...\")\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(df_fin['clean_desc'])\n",
    "    \n",
    "    # Get feature names and boost discovered industrial terms\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    industrial_indices = [i for i, term in enumerate(feature_names) if term in industrial_terms]\n",
    "    \n",
    "    # Enhance industrial term weights\n",
    "    if len(industrial_indices) > 0:\n",
    "        tfidf_matrix[:, industrial_indices] = tfidf_matrix[:, industrial_indices] * 2\n",
    "    \n",
    "    # Convert to dense array and combine with embeddings\n",
    "    tfidf_dense = tfidf_matrix.toarray()\n",
    "    combined_features = np.hstack([embeddings, tfidf_dense])\n",
    "    \n",
    "    # Perform clustering with combined features\n",
    "    print(f\"Clustering with {n_clusters} clusters...\")\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, \n",
    "                            batch_size=1000, n_init=3, max_iter=100)\n",
    "    cluster_labels = kmeans.fit_predict(combined_features)\n",
    "    \n",
    "    # Multi-stage cluster merging\n",
    "    print(\"Performing multi-stage cluster merging...\")\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # Stage 1: Merge based on semantic similarity (embedding part)\n",
    "    embedding_centroids = centroids[:, :embeddings.shape[1]]\n",
    "    similarity_matrix = cosine_similarity(embedding_centroids)\n",
    "    \n",
    "    to_merge = {}\n",
    "    for i in range(n_clusters):\n",
    "        for j in range(i+1, n_clusters):\n",
    "            if similarity_matrix[i, j] > similarity_threshold:\n",
    "                if i not in to_merge:\n",
    "                    to_merge[i] = j\n",
    "    \n",
    "    # Apply stage 1 merging\n",
    "    new_labels = cluster_labels.copy()\n",
    "    for old_cluster, new_cluster in to_merge.items():\n",
    "        new_labels[new_labels == old_cluster] = new_cluster\n",
    "    \n",
    "    # Stage 2: Merge based on keyword similarity (TF-IDF part)\n",
    "    cluster_keywords = {}\n",
    "    for cluster_id in np.unique(new_labels):\n",
    "        cluster_indices = np.where(new_labels == cluster_id)[0]\n",
    "        if len(cluster_indices) > 0:\n",
    "            # Get top keywords for this cluster\n",
    "            cluster_tfidf = tfidf_matrix[cluster_indices].sum(axis=0).A1\n",
    "            top_keyword_indices = np.argsort(cluster_tfidf)[-5:]  # Top 5 keywords\n",
    "            cluster_keywords[cluster_id] = set(feature_names[i] for i in top_keyword_indices \n",
    "                                             if cluster_tfidf[i] > 0)\n",
    "    \n",
    "    # Merge clusters with similar keywords\n",
    "    for i, cluster_i in enumerate(cluster_keywords.keys()):\n",
    "        for j, cluster_j in enumerate(cluster_keywords.keys()):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            common_keywords = cluster_keywords[cluster_i] & cluster_keywords[cluster_j]\n",
    "            if len(common_keywords) >= 3:  # At least 3 common keywords\n",
    "                new_labels[new_labels == cluster_j] = cluster_i\n",
    "    \n",
    "    # Renumber clusters\n",
    "    unique_clusters = np.unique(new_labels)\n",
    "    mapping = {old: new for new, old in enumerate(unique_clusters)}\n",
    "    cluster_labels = np.array([mapping[label] for label in new_labels])\n",
    "    n_final_clusters = len(unique_clusters)\n",
    "    print(f\"Final cluster count: {n_final_clusters}\")\n",
    "    \n",
    "    # Generate cluster names with multi-language support\n",
    "    print(\"Generating cluster names...\")\n",
    "    cluster_names = {}\n",
    "    \n",
    "    for cluster_id in range(n_final_clusters):\n",
    "        cluster_descs = df_fin['material_description'][cluster_labels == cluster_id].tolist()\n",
    "        if not cluster_descs:\n",
    "            cluster_names[cluster_id] = f\"cluster_{cluster_id}\"\n",
    "            continue\n",
    "        \n",
    "        # Extract terms from all languages\n",
    "        all_terms = []\n",
    "        for desc in cluster_descs:\n",
    "            desc_clean = re.sub(r'[^\\w\\säöüßàâçéèêëîïôùûüÿæøå]', ' ', desc, flags=re.IGNORECASE)\n",
    "            terms = desc_clean.lower().split()\n",
    "            terms = [term for term in terms if len(term) > 3]\n",
    "            all_terms.extend(terms)\n",
    "        \n",
    "        if not all_terms:\n",
    "            cluster_names[cluster_id] = \"_\".join(cluster_descs[0].split()[:3])\n",
    "            continue\n",
    "        \n",
    "        # Get most frequent terms\n",
    "        term_counts = pd.Series(all_terms).value_counts()\n",
    "        top_terms = term_counts.head(3).index.tolist()\n",
    "        cluster_names[cluster_id] = \"_\".join(top_terms)\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    df_fin['proposedkey'] = [cluster_names[label] for label in cluster_labels]\n",
    "    df_fin['cluster'] = cluster_labels\n",
    "    \n",
    "    # Automated validation and quality reporting\n",
    "    print(\"Performing automated validation...\")\n",
    "    validation_report = validate_clusters(df_fin, embeddings, cluster_labels, cluster_names)\n",
    "    \n",
    "    # Save output and validation report\n",
    "    output_cols = ['material_number', 'material_type', 'material_description', 'proposedkey', 'cluster']\n",
    "    output_file = f'advanced_output_k_{n_clusters}.csv'\n",
    "    df_fin[output_cols].to_csv(output_file, index=False)\n",
    "    \n",
    "    # Save validation report\n",
    "    report_file = f'validation_report_k_{n_clusters}.json'\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(validation_report, f, indent=2)\n",
    "    \n",
    "    # Print statistics\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    cluster_sizes = df_fin['cluster'].value_counts()\n",
    "    \n",
    "    print(f\"\\nCluster Statistics:\")\n",
    "    print(f\"Total clusters: {len(cluster_sizes)}\")\n",
    "    print(f\"Clusters with > 10 items: {len(cluster_sizes[cluster_sizes > 10])}\")\n",
    "    print(f\"Clusters with > 5 items: {len(cluster_sizes[cluster_sizes > 5])}\")\n",
    "    print(f\"Clusters with 1-2 items: {len(cluster_sizes[cluster_sizes <= 2])}\")\n",
    "    print(f\"Processing time: {total_time/60:.2f} minutes\")\n",
    "    print(f\"Output saved to {output_file}\")\n",
    "    print(f\"Validation report saved to {report_file}\")\n",
    "    \n",
    "    return df_fin, validation_report\n",
    "\n",
    "def validate_clusters(df, embeddings, cluster_labels, cluster_names):\n",
    "    \"\"\"\n",
    "    Automated cluster validation using multiple metrics\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        \"overall_metrics\": {},\n",
    "        \"cluster_quality\": {},\n",
    "        \"potential_issues\": []\n",
    "    }\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    try:\n",
    "        # Silhouette score (on a sample for large datasets)\n",
    "        sample_size = min(5000, len(embeddings))\n",
    "        sample_indices = np.random.choice(len(embeddings), sample_size, replace=False)\n",
    "        silhouette_avg = silhouette_score(embeddings[sample_indices], cluster_labels[sample_indices])\n",
    "        report[\"overall_metrics\"][\"silhouette_score\"] = silhouette_avg\n",
    "    except:\n",
    "        report[\"overall_metrics\"][\"silhouette_score\"] = \"Calculation failed\"\n",
    "    \n",
    "    # Calculate intra-cluster similarity\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    intra_cluster_similarities = []\n",
    "    \n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "        if len(cluster_indices) > 1:\n",
    "            cluster_embeddings = embeddings[cluster_indices]\n",
    "            centroid = np.mean(cluster_embeddings, axis=0)\n",
    "            similarities = cosine_similarity(cluster_embeddings, [centroid])\n",
    "            intra_cluster_similarities.append(np.mean(similarities))\n",
    "    \n",
    "    report[\"overall_metrics\"][\"avg_intra_cluster_similarity\"] = np.mean(intra_cluster_similarities) if intra_cluster_similarities else 0\n",
    "    \n",
    "    # Analyze each cluster\n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_data = df[cluster_labels == cluster_id]\n",
    "        cluster_size = len(cluster_data)\n",
    "        \n",
    "        if cluster_size == 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate cluster cohesion\n",
    "        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "        cluster_embeddings = embeddings[cluster_indices]\n",
    "        \n",
    "        if len(cluster_embeddings) > 1:\n",
    "            centroid = np.mean(cluster_embeddings, axis=0)\n",
    "            similarities = cosine_similarity(cluster_embeddings, [centroid])\n",
    "            cohesion = np.mean(similarities)\n",
    "        else:\n",
    "            cohesion = 1  # Single-item cluster\n",
    "            \n",
    "        # Check for potential issues\n",
    "        issues = []\n",
    "        if cluster_size == 1:\n",
    "            issues.append(\"singleton_cluster\")\n",
    "        elif cohesion < 0.5:\n",
    "            issues.append(\"low_cohesion\")\n",
    "            \n",
    "        report[\"cluster_quality\"][str(cluster_id)] = {\n",
    "            \"size\": cluster_size,\n",
    "            \"cohesion\": float(cohesion),\n",
    "            \"name\": cluster_names.get(cluster_id, \"unknown\"),\n",
    "            \"issues\": issues\n",
    "        }\n",
    "        \n",
    "        # Add to potential issues list\n",
    "        if issues:\n",
    "            report[\"potential_issues\"].append({\n",
    "                \"cluster_id\": cluster_id,\n",
    "                \"issues\": issues,\n",
    "                \"size\": cluster_size,\n",
    "                \"cohesion\": float(cohesion)\n",
    "            })\n",
    "    \n",
    "    # Calculate cluster size distribution\n",
    "    cluster_sizes = [info[\"size\"] for info in report[\"cluster_quality\"].values()]\n",
    "    report[\"overall_metrics\"][\"avg_cluster_size\"] = np.mean(cluster_sizes)\n",
    "    report[\"overall_metrics\"][\"max_cluster_size\"] = np.max(cluster_sizes)\n",
    "    report[\"overall_metrics\"][\"min_cluster_size\"] = np.min(cluster_sizes)\n",
    "    \n",
    "    return report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the advanced clustering\n",
    "    result, report = advanced_clustering(\n",
    "        input_file='input.csv', \n",
    "        material_type='FIN', \n",
    "        n_clusters=1000, \n",
    "        similarity_threshold=0.8\n",
    "    )\n",
    "    \n",
    "    # Print summary of validation results\n",
    "    print(\"\\nValidation Summary:\")\n",
    "    print(f\"Silhouette Score: {report['overall_metrics'].get('silhouette_score', 'N/A')}\")\n",
    "    print(f\"Average Intra-Cluster Similarity: {report['overall_metrics'].get('avg_intra_cluster_similarity', 'N/A'):.3f}\")\n",
    "    print(f\"Potential Issues Found: {len(report['potential_issues'])}\")\n",
    "    \n",
    "    # Show top 5 largest clusters\n",
    "    cluster_quality = report['cluster_quality']\n",
    "    largest_clusters = sorted(cluster_quality.items(), key=lambda x: x[1]['size'], reverse=True)[:5]\n",
    "    print(\"\\nTop 5 Largest Clusters:\")\n",
    "    for cluster_id, info in largest_clusters:\n",
    "        print(f\"Cluster {cluster_id} ({info['name']}): {info['size']} items, cohesion: {info['cohesion']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
