{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c83a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import time\n",
    "\n",
    "def enhanced_clustering(input_file, material_type='FIN', n_clusters=1000, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Enhanced clustering with better multilingual support using existing models\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting enhanced clustering for {material_type}\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(input_file)\n",
    "    df_fin = df[df['material_type'] == material_type].copy().reset_index(drop=True)\n",
    "    print(f\"Found {len(df_fin)} {material_type} records\")\n",
    "    \n",
    "    # Enhanced text cleaning that preserves multilingual characters\n",
    "    def clean_text(text):\n",
    "        text = str(text)\n",
    "        # Keep alphanumeric, spaces, and common multilingual characters\n",
    "        text = re.sub(r'[^\\w\\säöüßàâçéèêëîïôùûüÿæøå]', ' ', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text.lower()\n",
    "    \n",
    "    df_fin['clean_desc'] = df_fin['material_description'].apply(clean_text)\n",
    "    descriptions = df_fin['clean_desc'].tolist()\n",
    "    \n",
    "    # Create a manual multilingual synonym dictionary\n",
    "    print(\"Creating enhanced synonym mapping...\")\n",
    "    \n",
    "    # Common multilingual industrial term mappings\n",
    "    multilingual_mappings = {\n",
    "        # English to German/French common industrial terms\n",
    "        'sensor': ['sensor', 'fühler', 'geber', 'capteur'],\n",
    "        'unit': ['unit', 'einheit', 'unité', 'gerät'],\n",
    "        'cable': ['cable', 'kabel', 'câble', 'leitung'],\n",
    "        'connector': ['connector', 'stecker', 'connecteur', 'anschluss'],\n",
    "        'module': ['module', 'modul', 'module', 'baugruppe'],\n",
    "        'adapter': ['adapter', 'adapter', 'adaptateur', 'zwischenstück'],\n",
    "        'assembly': ['assembly', 'baugruppe', 'assemblage', 'montage'],\n",
    "        'assy': ['assy', 'baugr', 'ens', 'mont'],\n",
    "    }\n",
    "    \n",
    "    # Create reverse mapping for all terms\n",
    "    expanded_mapping = {}\n",
    "    for english_term, variants in multilingual_mappings.items():\n",
    "        for variant in variants:\n",
    "            expanded_mapping[variant] = english_term\n",
    "    \n",
    "    # Get unique tokens from data\n",
    "    all_tokens = set()\n",
    "    sample_size = min(10000, len(descriptions))\n",
    "    for desc in descriptions[:sample_size]:\n",
    "        all_tokens.update(desc.split())\n",
    "    all_tokens = list(all_tokens)\n",
    "    \n",
    "    # Enhanced token mapping with multilingual support\n",
    "    token_mapping = {}\n",
    "    for token in all_tokens:\n",
    "        # First check if it's in our multilingual dictionary\n",
    "        if token in expanded_mapping:\n",
    "            token_mapping[token] = expanded_mapping[token]\n",
    "            continue\n",
    "            \n",
    "        # For short tokens (abbreviations), use higher threshold\n",
    "        if len(token) <= 4:\n",
    "            matches = process.extract(token, all_tokens, scorer=fuzz.token_sort_ratio, limit=10)\n",
    "            best_match = token\n",
    "            for match, score, _ in matches:\n",
    "                if score > 90:  # High threshold for short words\n",
    "                    best_match = match\n",
    "                    break\n",
    "        else:\n",
    "            matches = process.extract(token, all_tokens, scorer=fuzz.token_sort_ratio, limit=5)\n",
    "            best_match = token\n",
    "            for match, score, _ in matches:\n",
    "                if score > 85 and len(match) < len(best_match):\n",
    "                    best_match = match\n",
    "        token_mapping[token] = best_match\n",
    "    \n",
    "    # Apply synonym normalization\n",
    "    print(\"Applying enhanced normalization...\")\n",
    "    def normalize_tokens(desc):\n",
    "        tokens = desc.split()\n",
    "        normalized = [token_mapping.get(token, token) for token in tokens]\n",
    "        return ' '.join(normalized)\n",
    "    \n",
    "    df_fin['normalized_desc'] = df_fin['clean_desc'].apply(normalize_tokens)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"Generating embeddings...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    batch_size = 2000\n",
    "    embeddings = []\n",
    "    for i in range(0, len(df_fin), batch_size):\n",
    "        batch_descs = df_fin['normalized_desc'].iloc[i:i+batch_size].tolist()\n",
    "        batch_embeddings = model.encode(batch_descs, show_progress_bar=False)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {min(i+batch_size, len(df_fin))}/{len(df_fin)}\")\n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Add TF-IDF features to help with multilingual matching\n",
    "    print(\"Adding multilingual features...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer='char', \n",
    "        ngram_range=(2, 4),  # Character n-grams work across languages\n",
    "        max_features=500\n",
    "    )\n",
    "    tfidf_features = vectorizer.fit_transform(df_fin['normalized_desc']).toarray()\n",
    "    \n",
    "    # Combine embeddings with TF-IDF features\n",
    "    combined_features = np.hstack([embeddings, tfidf_features])\n",
    "    \n",
    "    # Perform clustering\n",
    "    print(f\"Clustering with K={n_clusters}...\")\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, \n",
    "                            batch_size=1000, n_init=3, max_iter=100)\n",
    "    cluster_labels = kmeans.fit_predict(combined_features)\n",
    "    \n",
    "    # Enhanced merging with multilingual consideration\n",
    "    print(\"Multilingual-aware cluster merging...\")\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    similarity_matrix = cosine_similarity(centroids)\n",
    "    \n",
    "    to_merge = {}\n",
    "    for i in range(n_clusters):\n",
    "        for j in range(i+1, n_clusters):\n",
    "            if similarity_matrix[i, j] > similarity_threshold:\n",
    "                if i not in to_merge:\n",
    "                    to_merge[i] = j\n",
    "    \n",
    "    # Apply merging\n",
    "    new_labels = cluster_labels.copy()\n",
    "    for old_cluster, new_cluster in to_merge.items():\n",
    "        new_labels[new_labels == old_cluster] = new_cluster\n",
    "    \n",
    "    # Renumber clusters\n",
    "    unique_clusters = np.unique(new_labels)\n",
    "    mapping = {old: new for new, old in enumerate(unique_clusters)}\n",
    "    cluster_labels = np.array([mapping[label] for label in new_labels])\n",
    "    n_final_clusters = len(unique_clusters)\n",
    "    print(f\"Merged to {n_final_clusters} clusters\")\n",
    "    \n",
    "    # Enhanced cluster naming with multilingual support\n",
    "    print(\"Generating multilingual cluster names...\")\n",
    "    cluster_names = {}\n",
    "    \n",
    "    for cluster_id in range(n_final_clusters):\n",
    "        cluster_descs = df_fin['material_description'][cluster_labels == cluster_id].tolist()\n",
    "        if not cluster_descs:\n",
    "            cluster_names[cluster_id] = f\"cluster_{cluster_id}\"\n",
    "            continue\n",
    "        \n",
    "        # Extract terms with multilingual support\n",
    "        all_terms = []\n",
    "        for desc in cluster_descs:\n",
    "            # Clean but preserve multilingual characters\n",
    "            desc_clean = re.sub(r'[^\\w\\säöüßàâçéèêëîïôùûüÿæøå]', ' ', desc, flags=re.IGNORECASE)\n",
    "            terms = desc_clean.lower().split()\n",
    "            terms = [term for term in terms if len(term) > 2]  # Keep shorter terms for multilingual\n",
    "            all_terms.extend(terms)\n",
    "        \n",
    "        if not all_terms:\n",
    "            cluster_names[cluster_id] = \"_\".join(cluster_descs[0].split()[:3])\n",
    "            continue\n",
    "        \n",
    "        # Get most frequent terms\n",
    "        term_counts = pd.Series(all_terms).value_counts()\n",
    "        top_terms = term_counts.head(3).index.tolist()\n",
    "        cluster_names[cluster_id] = \"_\".join(top_terms)\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    df_fin['proposedkey'] = [cluster_names[label] for label in cluster_labels]\n",
    "    df_fin['cluster'] = cluster_labels\n",
    "    \n",
    "    # Automated validation for multilingual clusters\n",
    "    print(\"Running automated multilingual validation...\")\n",
    "    validation_results = validate_multilingual_clusters(df_fin, cluster_labels)\n",
    "    \n",
    "    # Save output with validation info\n",
    "    output_cols = ['material_number', 'material_type', 'material_description', 'proposedkey', 'cluster']\n",
    "    output_file = f'enhanced_output_k_{n_clusters}.csv'\n",
    "    df_fin[output_cols].to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print validation results\n",
    "    print(f\"\\nMultilingual Validation Results:\")\n",
    "    print(f\"Clusters with mixed languages: {validation_results['mixed_language_clusters']}\")\n",
    "    print(f\"Average cluster cohesion: {validation_results['avg_cohesion']:.3f}\")\n",
    "    print(f\"Potential issues: {len(validation_results['potential_issues'])}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    cluster_sizes = df_fin['cluster'].value_counts()\n",
    "    \n",
    "    print(f\"\\nCluster Statistics:\")\n",
    "    print(f\"Total clusters: {len(cluster_sizes)}\")\n",
    "    print(f\"Processing time: {total_time/60:.2f} minutes\")\n",
    "    print(f\"Output saved to {output_file}\")\n",
    "    \n",
    "    return df_fin\n",
    "\n",
    "def validate_multilingual_clusters(df, cluster_labels):\n",
    "    \"\"\"\n",
    "    Automated validation for multilingual clustering\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'mixed_language_clusters': 0,\n",
    "        'avg_cohesion': 0,\n",
    "        'potential_issues': []\n",
    "    }\n",
    "    \n",
    "    # Simple language detection patterns\n",
    "    german_patterns = ['ä', 'ö', 'ü', 'ß', 'sch', 'ch', 'gen', 'ung']\n",
    "    french_patterns = ['é', 'è', 'ê', 'à', 'ç', 'tion', 'ement', 'que']\n",
    "    english_patterns = ['ing', 'tion', 'ment', 'able', 'ize']\n",
    "    \n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    cohesion_scores = []\n",
    "    \n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_data = df[cluster_labels == cluster_id]\n",
    "        if len(cluster_data) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Check for mixed languages\n",
    "        languages = set()\n",
    "        for desc in cluster_data['material_description']:\n",
    "            desc_lower = desc.lower()\n",
    "            has_german = any(pattern in desc_lower for pattern in german_patterns)\n",
    "            has_french = any(pattern in desc_lower for pattern in french_patterns)\n",
    "            has_english = any(pattern in desc_lower for pattern in english_patterns)\n",
    "            \n",
    "            if has_german: languages.add('german')\n",
    "            if has_french: languages.add('french')\n",
    "            if has_english: languages.add('english')\n",
    "            if not any([has_german, has_french, has_english]): languages.add('unknown')\n",
    "        \n",
    "        if len(languages) > 1:\n",
    "            results['mixed_language_clusters'] += 1\n",
    "            results['potential_issues'].append({\n",
    "                'cluster_id': cluster_id,\n",
    "                'issue': f'mixed_languages: {languages}',\n",
    "                'size': len(cluster_data)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the enhanced clustering\n",
    "    result = enhanced_clustering(\n",
    "        input_file='input.csv', \n",
    "        material_type='FIN', \n",
    "        n_clusters=1000, \n",
    "        similarity_threshold=0.8\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
